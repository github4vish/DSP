{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18c548a-98f8-4bee-be65-bbfbc03e1659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "Confusion Matrix: [[0 1]\n",
      " [0 2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sceta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('reviews.csv')\n",
    "\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "data['clean_text'] = data['review'].apply(preprocess)\n",
    "\n",
    "\n",
    "# Feature extraction\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(data['clean_text'])\n",
    "y = data['sentiment']\n",
    "\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b36d85e2-d983-4aaa-bb20-9b9bee764683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sceta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sceta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Sample dataset\n",
    "text = [\n",
    "\"I love this product, it is amazing!\",\n",
    "\"This is the worst thing I ever bought\",\n",
    "\"Absolutely fantastic experience\",\n",
    "\"Totally disappointing and horrible\",\n",
    "\"I enjoyed using it very much\",\n",
    "\"I hate it, very bad quality\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 0] # 1 = Positive, 0 = Negative\n",
    "\n",
    "\n",
    "# Text preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess(sentence):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    words = [stemmer.stem(w) for w in words if w not in stop_words and w.isalpha()]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "processed_text = [preprocess(t) for t in text]\n",
    "\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(processed_text)\n",
    "\n",
    "\n",
    "y = labels\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Model Training\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Prediction\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc6c60fe-41e0-4b71-b2c2-262a23a4a10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n",
      ".\n",
      "Part of Speech Tags:\n",
      "Apple -- PROPN\n",
      "is -- AUX\n",
      "looking -- VERB\n",
      "at -- ADP\n",
      "buying -- VERB\n",
      "U.K. -- PROPN\n",
      "startup -- VERB\n",
      "for -- ADP\n",
      "$ -- SYM\n",
      "1 -- NUM\n",
      "billion -- NUM\n",
      ". -- PUNCT\n",
      "Named Entities:\n",
      "Apple | ORG\n",
      "U.K. | GPE\n",
      "$1 billion | MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n",
    "\n",
    "# ----- Tokenization -----\n",
    "print(\"Tokens:\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n",
    "# ----- Part of Speech Tagging -----\n",
    "print(\"Part of Speech Tags:\")\n",
    "for token in doc:\n",
    "    print(token.text, \"--\", token.pos_)\n",
    "\n",
    "# ----- Named Entity Recognition -----\n",
    "print(\"Named Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c99c37-4b34-42ad-beb4-2f423d91c873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words: ['Natural', 'Language', 'Processing', 'with', 'NLTK', 'is', 'fun', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sceta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install NLTK using the command:\n",
    "# pip install nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Step 2: Download required corpora\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Step 3: Sample text\n",
    "text = \"Natural Language Processing with NLTK is fun!\"\n",
    "\n",
    "# Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(text)\n",
    "print(\"Tokenized Words:\", words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7289622f-aa54-4b57-a661-713b15a658b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          review sentiment Predicted_Sentiment\n",
      "0   The product is excellent and works perfectly  positive            positive\n",
      "1             I am very happy with this purchase  positive            positive\n",
      "2                Great quality and fast delivery  positive            positive\n",
      "3           This is the best product I have used  positive            positive\n",
      "4                Absolutely fantastic experience  positive            positive\n",
      "5            The product is terrible and useless  negative            negative\n",
      "6        I am very disappointed with the quality  negative            negative\n",
      "7                       Worst purchase ever made  negative            negative\n",
      "8                     Not worth the money at all  negative            negative\n",
      "9         The item stopped working after one day  negative             neutral\n",
      "10                          Good value for money  positive            positive\n",
      "11         Really satisfied with the performance  positive            positive\n",
      "12                  Poor design and bad material  negative            negative\n",
      "13                 Highly recommend this product  positive            positive\n",
      "14                     Completely waste of money  negative            negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Step 1: Load dataset\n",
    "data = pd.read_csv(\"Reviews.csv\")\n",
    "\n",
    "# Step 2: Define sentiment lexicon\n",
    "positive_words = [\"excellent\", \"happy\", \"great\", \"best\", \"fantastic\", \"good\", \"satisfied\", \"recommend\", \"perfectly\"]\n",
    "\n",
    "negative_words = [\"terrible\", \"useless\", \"disappointed\", \"worst\", \"not\", \"poor\", \"bad\", \"waste\"]\n",
    "\n",
    "# Step 3: Sentiment analysis function\n",
    "def analyze_sentiment(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "\n",
    "    for word in tokens:\n",
    "        if word in positive_words:\n",
    "            pos += 1\n",
    "        elif word in negative_words:\n",
    "            neg += 1\n",
    "\n",
    "    if pos > neg:\n",
    "        return \"positive\"\n",
    "    elif neg > pos:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Step 4: Apply sentiment analysis\n",
    "data[\"Predicted_Sentiment\"] = data[\"review\"].apply(analyze_sentiment)\n",
    "\n",
    "# Step 5: Display results\n",
    "print(data[[\"review\", \"sentiment\", \"Predicted_Sentiment\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7495d4b-0ac9-4667-98cb-efcb0111049c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
